some config:
data_dir = ./data
output_dir = ./output
embedding_path = ./embedding/hlbl-embeddings-scaled.EMBEDDING_SIZE=50.txt
word_dim = 50
model_name = CNN
mode = 1
seed = 5782
cuda = 0
epoch = 20
dropout = 0.5
batch_size = 128
lr = 0.001
max_len = 100
pos_dis = 50
pos_dim = 5
hidden_size = 100
filter_num = 200
window = 3
L2_decay = 1e-05
device = cuda:0
model_dir = ./output/CNN
--------------------------------------
start to load data ...
finish!
--------------------------------------
CNN(
  (word_embedding): Embedding(246123, 50)
  (pos1_embedding): Embedding(103, 5)
  (pos2_embedding): Embedding(103, 5)
  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))
  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)
  (tanh): Tanh()
  (dropout): Dropout(p=0.5, inplace=False)
  (linear): Linear(in_features=200, out_features=100, bias=True)
  (dense): Linear(in_features=100, out_features=19, bias=True)
)
traning model parameters:
word_embedding.weight :  torch.Size([246123, 50])
pos1_embedding.weight :  torch.Size([103, 5])
pos2_embedding.weight :  torch.Size([103, 5])
conv.weight :  torch.Size([200, 1, 3, 60])
conv.bias :  torch.Size([200])
linear.weight :  torch.Size([100, 200])
linear.bias :  torch.Size([100])
dense.weight :  torch.Size([19, 100])
dense.bias :  torch.Size([19])
12,365,399 total parameters.
--------------------------------------
start to train the model ...
开始时间/s 1650605383.0557826
[001] train_loss: 2.445 | dev_loss: 2.448 | micro f1 on dev: 0.5182 | pre on dev: 0.6591 | recall on dev: 0.5488 >>> save models!
[002] train_loss: 1.842 | dev_loss: 1.835 | micro f1 on dev: 0.4142 | pre on dev: 0.4171 | recall on dev: 0.6412 
[003] train_loss: 1.393 | dev_loss: 1.414 | micro f1 on dev: 0.5751 | pre on dev: 0.5521 | recall on dev: 0.6491 >>> save models!
[004] train_loss: 1.175 | dev_loss: 1.256 | micro f1 on dev: 0.6663 | pre on dev: 0.6964 | recall on dev: 0.6548 >>> save models!
[005] train_loss: 0.959 | dev_loss: 1.119 | micro f1 on dev: 0.6972 | pre on dev: 0.7315 | recall on dev: 0.6790 >>> save models!
[006] train_loss: 0.799 | dev_loss: 1.029 | micro f1 on dev: 0.7381 | pre on dev: 0.7577 | recall on dev: 0.7217 >>> save models!
[007] train_loss: 0.654 | dev_loss: 0.979 | micro f1 on dev: 0.7520 | pre on dev: 0.7671 | recall on dev: 0.7400 >>> save models!
[008] train_loss: 0.551 | dev_loss: 0.955 | micro f1 on dev: 0.7585 | pre on dev: 0.7766 | recall on dev: 0.7462 >>> save models!
[009] train_loss: 0.455 | dev_loss: 0.928 | micro f1 on dev: 0.7619 | pre on dev: 0.7804 | recall on dev: 0.7462 >>> save models!
[010] train_loss: 0.373 | dev_loss: 0.903 | micro f1 on dev: 0.7660 | pre on dev: 0.7709 | recall on dev: 0.7642 >>> save models!
[011] train_loss: 0.297 | dev_loss: 0.891 | micro f1 on dev: 0.7715 | pre on dev: 0.7768 | recall on dev: 0.7682 >>> save models!
[012] train_loss: 0.250 | dev_loss: 0.905 | micro f1 on dev: 0.7780 | pre on dev: 0.7915 | recall on dev: 0.7670 >>> save models!
[013] train_loss: 0.218 | dev_loss: 0.937 | micro f1 on dev: 0.7823 | pre on dev: 0.8160 | recall on dev: 0.7522 >>> save models!
[014] train_loss: 0.160 | dev_loss: 0.935 | micro f1 on dev: 0.7822 | pre on dev: 0.7951 | recall on dev: 0.7715 
[015] train_loss: 0.140 | dev_loss: 0.969 | micro f1 on dev: 0.7820 | pre on dev: 0.8106 | recall on dev: 0.7566 
[016] train_loss: 0.109 | dev_loss: 0.980 | micro f1 on dev: 0.7842 | pre on dev: 0.8107 | recall on dev: 0.7608 >>> save models!
[017] train_loss: 0.089 | dev_loss: 0.998 | micro f1 on dev: 0.7837 | pre on dev: 0.8070 | recall on dev: 0.7634 
[018] train_loss: 0.076 | dev_loss: 1.036 | micro f1 on dev: 0.7858 | pre on dev: 0.8141 | recall on dev: 0.7605 >>> save models!
[019] train_loss: 0.054 | dev_loss: 1.025 | micro f1 on dev: 0.7868 | pre on dev: 0.8067 | recall on dev: 0.7684 >>> save models!
[020] train_loss: 0.046 | dev_loss: 1.055 | micro f1 on dev: 0.7846 | pre on dev: 0.8084 | recall on dev: 0.7628 
--------------------------------------
start test ...
测试开始时间/s 1650605415.0504243
test_loss: 1.025 | macro f1 on test:  0.7868 | p: 0.8067 | r: 0.7684
结束时间/s 1650605415.3008194